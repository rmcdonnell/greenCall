{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the \"greenCall\" python package\n",
    "\n",
    "At this point in time, the greenCall python package requires a series of function calls \n",
    "to make our way through the data pipeline. The pipeline consists of the following:\n",
    "\n",
    "1. Read the csv file formatted as (unique id, query term)\n",
    "2. Request information from the Search API \n",
    "3. Write results to disk in JSON format\n",
    "4. Bulk upload results to elasticsearch\n",
    "\n",
    "This notebook provides a concise example of how to work through the data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maximum number of query items to request from API\n",
    "QUERY_LIMIT = 20\n",
    "\n",
    "# Maximum number or requests deferred\n",
    "MAX_RUN = 20\n",
    "\n",
    "# This many seconds will expire between requests sent\n",
    "RATE_LIMIT = 1\n",
    "\n",
    "# Path to original excel file, converted to CSV\n",
    "filepath = 'examples/finance_demo.csv'\n",
    "\n",
    "# Path to converted file to be used for API requests\n",
    "outpath = 'examples/ipython_demo.json'\n",
    "\n",
    "# results returned from the API via the networking engine\n",
    "resultspath = 'results.json'\n",
    "\n",
    "# Specify a document template for Elasticsearch\n",
    "esformat = {\n",
    "            \"_index\": \"ipythonsearch\",\n",
    "            \"_type\": \"website\",\n",
    "            \"_id\": None,\n",
    "            \"_source\": \"\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Start Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from greencall.utils.utilityBelt import enable_log\n",
    "\n",
    "# Log everything, always.\n",
    "enable_log('crawlah')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 (Reading the CSV file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from greencall.csvclean.inputCsv import tojson\n",
    "\n",
    "# Convert the input file from CSV to JSON\n",
    "tojson(filepath, outpath, QUERY_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ( Request information from the Search API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from greencall.csvclean.clientConversion import runConversion\n",
    "from examples.secret import secret_key\n",
    "\n",
    "# Use the API client to convert query terms into correct format\n",
    "# for API requests. Currently hard coded for Google Search API\n",
    "adict = runConversion(jsonpath=outpath,\n",
    "                      secretKey= secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 (Write results to disk in JSON format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from twisted.internet import reactor\n",
    "from greencall.crawlah import getPages\n",
    "\n",
    "# Load the network engine which handles API requests (gas & brakes)\n",
    "gp = getPages(adict, MAX_RUN, RATE_LIMIT)\n",
    "\n",
    "# Start the networking engine\n",
    "\n",
    "gp.start()\n",
    "reactor.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 (Bulk upload into elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import string\n",
    "import json\n",
    "from elasticsearch import Elasticsearch,helpers\n",
    "\n",
    "from greencall.utils.loadelastic import read_json, prepare_all_documents\n",
    "from greencall.csvclean.inputCsv import read_csv\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "# Read results from the Search API\n",
    "results = read_json(resultspath)\n",
    "\n",
    "# Take the length of the results as a sanity check\n",
    "len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# revising so the data types are correct\n",
    "for key in results.keys():\n",
    "    key = int(key) # this is wrong btw; didn't assign back to dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "6020\n",
      "28800\n",
      "1280\n",
      "1790\n",
      "3\n",
      "4\n",
      "3\n",
      "0\n",
      "4680\n",
      "62800\n",
      "0\n",
      "4780\n",
      "1950\n",
      "0\n",
      "433\n",
      "2550\n",
      "0\n",
      "517\n",
      "4730\n"
     ]
    }
   ],
   "source": [
    "# total number of items returned is initially limited to 10\n",
    "# given 20 items, here's the total number of search items availalble\n",
    "\n",
    "for key in results:\n",
    "    print json.loads(results[key])['queries']['request'][0]['totalResults']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare all documents for bulk upload into elasticsearch\n",
    "actions = prepare_all_documents(results, esformat, read_csv(filepath, QUERY_LIMIT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with search api to elasticsearch document converter\n",
    "\n",
    "1. Function returns list value even though the last contains a dictionary\n",
    "2. Too many generic fields from Google Custom Search API are returned as separate documents\n",
    "3. Parent-child relationships are not mapped\n",
    "4. Generic document converter is used, may need some google-specific logic to handle conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it looks like the search api to elasticsearch document converter needs the most attention\n",
    "len(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account_holder': 'Walter Broughton',\n",
       " 'account_number': u'11111417',\n",
       " u'searchInformation': [u'formattedSearchTime',\n",
       "  u'formattedTotalResults',\n",
       "  u'totalResults',\n",
       "  u'searchTime']}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's pretty clear from steping through the documents that\n",
    "# there are some issues with the parser. The first version\n",
    "# definitely worked better than nothing but we are going to\n",
    "# need to make some changes.\n",
    "\n",
    "actions[47]['_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: True, 10: True, 'null': 261, 4: True}\n"
     ]
    }
   ],
   "source": [
    "# trying to get a better idea of how the documents were parsed\n",
    "\n",
    "nums = {}\n",
    "count = 0\n",
    "for action in actions:\n",
    "    if 'items' in action['_source']:\n",
    "        nums[len(action['_source']['items'])]= True\n",
    "    else:\n",
    "        nums['null'] = count\n",
    "        count += 1\n",
    "    \n",
    "print nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert list to generator (low level detail)\n",
    "actions = iter(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "BulkIndexError",
     "evalue": "('1 document(s) failed to index.', [{u'index': {u'status': 400, u'_type': u'website', u'_id': u'24', u'error': u'MapperParsingException[failed to parse [items.pagemap.metatags.og:updated_time]]; nested: MapperParsingException[failed to parse date field [7/8/2015 10:07:34 AM], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"7/8/2015 10:07:34 AM\" is malformed at \"/8/2015 10:07:34 AM\"]; ', u'_index': u'ipythonsearch'}}])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBulkIndexError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-732b2697b0d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load elasticsearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbulk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/tbonza/.conda/envs/callgreen/lib/python2.7/site-packages/elasticsearch/helpers/__init__.pyc\u001b[0m in \u001b[0;36mbulk\u001b[1;34m(client, actions, stats_only, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstreaming_bulk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;31m# go through request-reponse pairs and detect failures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tbonza/.conda/envs/callgreen/lib/python2.7/site-packages/elasticsearch/helpers/__init__.pyc\u001b[0m in \u001b[0;36mstreaming_bulk\u001b[1;34m(client, actions, chunk_size, raise_on_error, expand_action_callback, raise_on_exception, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBulkIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%i document(s) failed to index.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbulk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBulkIndexError\u001b[0m: ('1 document(s) failed to index.', [{u'index': {u'status': 400, u'_type': u'website', u'_id': u'24', u'error': u'MapperParsingException[failed to parse [items.pagemap.metatags.og:updated_time]]; nested: MapperParsingException[failed to parse date field [7/8/2015 10:07:34 AM], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"7/8/2015 10:07:34 AM\" is malformed at \"/8/2015 10:07:34 AM\"]; ', u'_index': u'ipythonsearch'}}])"
     ]
    }
   ],
   "source": [
    "# load elasticsearch \n",
    "\n",
    "helpers.bulk(es,actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some documents have failed to load because the date format is malformed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
