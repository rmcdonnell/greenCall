{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the \"greenCall\" python package\n",
    "\n",
    "At this point in time, the greenCall python package requires a series of function calls \n",
    "to make our way through the data pipeline. The pipeline consists of the following:\n",
    "\n",
    "1. Read the csv file formatted as (unique id, query term)\n",
    "2. Request information from the Search API \n",
    "3. Write results to disk in JSON format\n",
    "4. Bulk upload results to elasticsearch\n",
    "\n",
    "This notebook provides a concise example of how to work through the data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maximum number of query items to request from API\n",
    "QUERY_LIMIT = 20\n",
    "\n",
    "# Maximum number or requests deferred\n",
    "MAX_RUN = 20\n",
    "\n",
    "# This many seconds will expire between requests sent\n",
    "RATE_LIMIT = 1\n",
    "\n",
    "# Path to original excel file, converted to CSV\n",
    "filepath = 'examples/finance_demo.csv'\n",
    "\n",
    "# Path to converted file to be used for API requests\n",
    "outpath = 'examples/ipython_demo.json'\n",
    "\n",
    "# results returned from the API via the networking engine\n",
    "resultspath = 'results.json'\n",
    "\n",
    "# Specify a document template for Elasticsearch\n",
    "esformat = {\n",
    "            \"_index\": \"ipythonsearch\",\n",
    "            \"_type\": \"website\",\n",
    "            \"_id\": None,\n",
    "            \"_source\": \"\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Start Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from greencall.utils.utilityBelt import enable_log\n",
    "\n",
    "# Log everything, always.\n",
    "enable_log('crawlah')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 (Reading the CSV file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from greencall.csvclean.inputCsv import tojson\n",
    "\n",
    "# Convert the input file from CSV to JSON\n",
    "tojson(filepath, outpath, QUERY_LIMIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ( Request information from the Search API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from greencall.csvclean.clientConversion import runConversion\n",
    "from examples.secret import secret_key\n",
    "\n",
    "# Use the API client to convert query terms into correct format\n",
    "# for API requests. Currently hard coded for Google Search API\n",
    "adict = runConversion(jsonpath=outpath,\n",
    "                      secretKey= secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 (Write results to disk in JSON format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:[Failure instance: Traceback (failure with no frames): <class 'twisted.web.error.Error'>: 503 Service Unavailable\n",
      "]\n",
      "ERROR:root:[Failure instance: Traceback (failure with no frames): <class 'twisted.web.error.Error'>: 500 Internal Server Error\n",
      "]\n",
      "ERROR:root:[Failure instance: Traceback (failure with no frames): <class 'twisted.web.error.Error'>: 500 Internal Server Error\n",
      "]\n",
      "ERROR:root:[Failure instance: Traceback (failure with no frames): <class 'twisted.web.error.Error'>: 500 Internal Server Error\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from twisted.internet import reactor\n",
    "from greencall.crawlah import getPages\n",
    "\n",
    "# Load the network engine which handles API requests (gas & brakes)\n",
    "gp = getPages(adict, MAX_RUN, RATE_LIMIT)\n",
    "\n",
    "# Start the networking engine\n",
    "gp.start()\n",
    "reactor.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 (Bulk upload into elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import string\n",
    "import json\n",
    "from elasticsearch import Elasticsearch,helpers\n",
    "\n",
    "from greencall.utils.loadelastic import read_json, prepare_all_documents\n",
    "from greencall.csvclean.inputCsv import read_csv\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "# Read results from the Search API\n",
    "results = read_json(resultspath)\n",
    "\n",
    "# Take the length of the results as a sanity check\n",
    "len(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# revising so the data types are correct\n",
    "for key in results.keys():\n",
    "    key = int(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "26\n",
      "41\n",
      "56\n",
      "71\n",
      "85\n",
      "100\n",
      "114\n",
      "125\n",
      "140\n",
      "140\n",
      "151\n",
      "151\n",
      "166\n",
      "166\n",
      "180\n",
      "180\n",
      "194\n",
      "209\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "# prepare all documents for bulk upload into elasticsearch\n",
    "actions = prepare_all_documents(results, esformat, read_csv(filepath, QUERY_LIMIT))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert list to generator (low level detail)\n",
    "actions = iter(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "BulkIndexError",
     "evalue": "('2 document(s) failed to index.', [{u'index': {u'status': 400, u'_type': u'website', u'_id': u'18', u'error': u'MapperParsingException[failed to parse [items.pagemap.metatags.og:updated_time]]; nested: MapperParsingException[failed to parse date field [6/26/2015 10:09:00 AM], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"6/26/2015 10:09:00 AM\" is malformed at \"/26/2015 10:09:00 AM\"]; ', u'_index': u'ipythonsearch'}}, {u'index': {u'status': 400, u'_type': u'website', u'_id': u'26', u'error': u'MapperParsingException[failed to parse [items.pagemap.review.datepublished]]; nested: MapperParsingException[failed to parse date field [08/08/2013], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"08/08/2013\" is malformed at \"/08/2013\"]; ', u'_index': u'ipythonsearch'}}])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBulkIndexError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-732b2697b0d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load elasticsearch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbulk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/tbonza/.conda/envs/callgreen/lib/python2.7/site-packages/elasticsearch/helpers/__init__.pyc\u001b[0m in \u001b[0;36mbulk\u001b[1;34m(client, actions, stats_only, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mok\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstreaming_bulk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;31m# go through request-reponse pairs and detect failures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/tbonza/.conda/envs/callgreen/lib/python2.7/site-packages/elasticsearch/helpers/__init__.pyc\u001b[0m in \u001b[0;36mstreaming_bulk\u001b[1;34m(client, actions, chunk_size, raise_on_error, expand_action_callback, raise_on_exception, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mBulkIndexError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%i document(s) failed to index.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbulk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBulkIndexError\u001b[0m: ('2 document(s) failed to index.', [{u'index': {u'status': 400, u'_type': u'website', u'_id': u'18', u'error': u'MapperParsingException[failed to parse [items.pagemap.metatags.og:updated_time]]; nested: MapperParsingException[failed to parse date field [6/26/2015 10:09:00 AM], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"6/26/2015 10:09:00 AM\" is malformed at \"/26/2015 10:09:00 AM\"]; ', u'_index': u'ipythonsearch'}}, {u'index': {u'status': 400, u'_type': u'website', u'_id': u'26', u'error': u'MapperParsingException[failed to parse [items.pagemap.review.datepublished]]; nested: MapperParsingException[failed to parse date field [08/08/2013], tried both date format [dateOptionalTime], and timestamp number with locale []]; nested: IllegalArgumentException[Invalid format: \"08/08/2013\" is malformed at \"/08/2013\"]; ', u'_index': u'ipythonsearch'}}])"
     ]
    }
   ],
   "source": [
    "# load elasticsearch \n",
    "\n",
    "helpers.bulk(es,actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some documents have failed to load because the date format is malformed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
